{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20155\\AppData\\Local\\Temp\\ipykernel_10656\\1321522823.py:24: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.29166667 0.30555556 0.15277778 0.22222222 0.20833333 0.26388889\n",
      " 0.25694444 0.54861111 0.21527778 0.43055556 0.16666667 0.27777778\n",
      " 0.13888889 0.30555556 0.47222222 0.27777778 0.20833333 0.5\n",
      " 0.17361111 0.41666667 0.19444444 0.25       0.30555556 0.34722222\n",
      " 0.30555556 0.30555556 0.23611111 0.13888889 0.125      0.34722222\n",
      " 0.15277778 0.33333333 0.16666667 0.33333333 0.17361111 0.40277778\n",
      " 0.34722222 0.23611111 0.16666667 0.54166667 0.125      0.33333333\n",
      " 0.11111111 0.47222222 0.08333333 0.52777778 0.20138889 0.07638889\n",
      " 0.20833333 0.40277778 0.5        0.83333333 0.48611111 0.83333333\n",
      " 0.75       0.51388889 0.58333333 0.69444444 0.47222222 0.68055556\n",
      " 0.54166667 0.79166667 0.54166667 0.875      0.41666667 0.875\n",
      " 0.40277778 1.         0.58333333 0.73611111 0.83333333 0.63888889\n",
      " 0.47222222 0.65277778 0.63888889 0.66666667 0.38888889 0.81944444\n",
      " 0.52777778 0.79166667 0.70833333 0.55555556 0.65277778 0.93055556\n",
      " 0.68055556 0.75       0.625      0.70833333 0.45833333 0.73611111\n",
      " 0.34722222 0.83333333 0.44444444 0.56944444 0.79166667 0.61111111\n",
      " 0.41666667 0.75       0.66666667 0.72222222 0.22222222 0.33333333\n",
      " 0.26388889 0.22916667 0.28472222 0.34722222 0.15277778 0.29166667\n",
      " 0.40277778 0.27777778 0.30555556 0.29861111 0.27777778 0.375\n",
      " 0.24305556 0.375      0.16666667 0.27777778 0.20833333 0.47222222\n",
      " 0.25       0.19444444 0.05555556 0.30555556 0.16666667 0.40277778\n",
      " 0.19444444 0.30555556 0.27777778 0.51388889 0.13888889 0.44444444\n",
      " 0.18055556 0.38888889 0.25       0.33333333 0.31944444 0.58333333\n",
      " 0.         0.5        0.34722222 0.26388889 0.23611111 0.22222222\n",
      " 0.27083333 0.48611111 0.19444444 0.44444444 0.15277778 0.27083333]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:,1:5]=normalizer.fit_transform(data.iloc[:,1:5]).astype(float)\n",
      "C:\\Users\\20155\\AppData\\Local\\Temp\\ipykernel_10656\\1321522823.py:24: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.15517241 0.24137931 0.39655172 0.24137931 0.36206897 0.31034483\n",
      " 0.15517241 0.39655172 0.36206897 0.31034483 0.24137931 0.13793103\n",
      " 0.17241379 0.32758621 0.44827586 0.22413793 0.39655172 0.43103448\n",
      " 0.20689655 0.37931034 0.03448276 0.13793103 0.29310345 0.22413793\n",
      " 0.13793103 0.25862069 0.18965517 0.25862069 0.         0.13793103\n",
      " 0.10344828 0.10344828 0.27586207 0.20689655 0.39655172 0.4137931\n",
      " 0.31034483 0.13793103 0.15517241 0.20689655 0.17241379 0.39655172\n",
      " 0.24137931 0.4137931  0.22413793 0.31034483 0.17241379 0.12068966\n",
      " 0.31034483 0.32758621 0.67241379 1.         0.65517241 0.79310345\n",
      " 0.74137931 0.65517241 0.67241379 0.81034483 0.63793103 0.74137931\n",
      " 0.72413793 0.75862069 0.72413793 0.70689655 0.65517241 0.77586207\n",
      " 0.65517241 0.84482759 0.63793103 0.86206897 0.79310345 0.74137931\n",
      " 0.70689655 0.74137931 0.74137931 0.74137931 0.75862069 0.74137931\n",
      " 0.65517241 0.82758621 0.86206897 0.63793103 0.60344828 1.\n",
      " 0.82758621 0.82758621 0.70689655 0.81034483 0.62068966 0.62068966\n",
      " 0.62068966 0.9137931  0.65517241 0.75862069 0.86206897 0.77586207\n",
      " 0.65517241 0.9137931  0.70689655 0.74137931 0.34482759 0.4137931\n",
      " 0.36206897 0.27586207 0.43103448 0.44827586 0.10344828 0.43103448\n",
      " 0.39655172 0.44827586 0.36206897 0.37931034 0.22413793 0.5\n",
      " 0.31034483 0.5        0.43103448 0.15517241 0.31034483 0.39655172\n",
      " 0.15517241 0.32758621 0.25862069 0.36206897 0.39655172 0.43103448\n",
      " 0.48275862 0.48275862 0.32758621 0.56896552 0.25862069 0.5\n",
      " 0.25862069 0.53448276 0.39655172 0.46551724 0.39655172 0.65517241\n",
      " 0.34482759 0.56896552 0.65517241 0.25862069 0.4137931  0.4137931\n",
      " 0.4137931  0.5        0.31034483 0.68965517 0.25862069 0.44827586]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:,1:5]=normalizer.fit_transform(data.iloc[:,1:5]).astype(float)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import layer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split     \n",
    "\n",
    "\n",
    "data=pd.read_csv('birds.csv')\n",
    "gender_distribution = data.groupby('bird category')['gender'].apply(lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown')\n",
    "data['gender'] = data.apply(lambda row: gender_distribution[row['bird category']] if pd.isnull(row['gender']) else row['gender'], axis=1) \n",
    "label_encoder=preprocessing.LabelEncoder()\n",
    "data.iloc[:,0]=label_encoder.fit_transform(data.iloc[:,0])\n",
    "\n",
    "\n",
    "hot_encoder = preprocessing.OneHotEncoder(sparse_output=False) \n",
    "encoded_columns = hot_encoder.fit_transform(data.iloc[:,-1].values.reshape(-1,1))\n",
    "encoded_df = pd.DataFrame(encoded_columns, columns=hot_encoder.get_feature_names_out([data.columns[-1]]))\n",
    "\n",
    "\n",
    "# Drop the original last column and append one-hot-encoded columns\n",
    "data = pd.concat([data.drop(data.columns[-1], axis=1), encoded_df], axis=1)\n",
    "\n",
    "normalizer=preprocessing.MinMaxScaler()\n",
    "data.iloc[:,1:5]=normalizer.fit_transform(data.iloc[:,1:5]).astype(float)\n",
    "X=data.iloc[:,0:5]\n",
    "Y=data.iloc[:,5:]\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.4,stratify=Y,shuffle=True)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we need to create layer \n",
    "- we need in all layer n neurans "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### layer \n",
    "\n",
    "layer is list have n neurans work seperated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we need to create single neuran fist then make layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make matrix for W (weight) it will by n*m\n",
    "- where n is number of features\n",
    "- m is number of neurans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>[0.21 , -0.4]<br>[0.15 , 0.1]<p> previous matrix is W \n",
    "\n",
    "- where first col is w1\n",
    "- second col is w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first layer \n",
    "# W = np.random.rand(data.shape , units)\n",
    "# bias = np.random.rand(units)\n",
    "X =np.array([0,0])\n",
    "W = np.array([[0.21,-0.4],\n",
    "              [0.15,0.1]])\n",
    "bias = np.array([-0.3,0.25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return round((1/(1+math.exp(-Z))) , 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "#  a_out = np.zeros(units)\n",
    "#  for j in range(units):\n",
    "#   Z = np.dot(X , W[: , j]) + bias[j]\n",
    "#   a_out[j] = activation(function_name , Z)\n",
    "\n",
    "Z_1 = np.dot(X , W[: , 0]) + bias[0]\n",
    "Z_2 = np.dot(X , W[: , 1])+bias[1]\n",
    "print(Z_1)\n",
    "print(Z_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welcome\n"
     ]
    }
   ],
   "source": [
    "welcome = \"WelCome\"\n",
    "print(welcome.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hideen_Layer:\n",
    "    def __init__(self , neurons , activation_function , data):\n",
    "        self.neurons = int(neurons)\n",
    "        self.activation_function =activation_function\n",
    "        self.data = data\n",
    "        # W = np.random.rand(data.shape[1] ,  self.neurons)\n",
    "        self.W = np.random.rand(self.data.shape[0] , self.neurons)\n",
    "        self.bias = np.random.rand(self.neurons)\n",
    "        self.a_out= np.zeros(self.neurons) # output from each neuron (unit)\n",
    "        self.error = np.zeros(self.neurons)\n",
    "        self.differentiating = np.zeros(self.neurons)\n",
    "        self.a = 1\n",
    "        self.b = 1\n",
    "        \n",
    "        self.train()\n",
    "    # activation function & their differentiating\n",
    "    #%% \n",
    "    def sigmoid(self,Z , a =1):\n",
    "        self.a = a\n",
    "        return (1/(1+math.exp(-a*Z)))\n",
    "    \n",
    "    def sigmoid_differentiating(self,Z , a = 1):\n",
    "        self.a = a\n",
    "        y =self.sigmoid(Z,  self.a)\n",
    "        \n",
    "        return self.a*y*(1-y)\n",
    "    \n",
    "    # did you need to store a and b ??\n",
    "    \n",
    "    def hyperbolic_tangent(self,Z , b =1):\n",
    "        self.b = b\n",
    "        top = 1-math.exp(- self.b*Z)\n",
    "        down = 1 + math.exp(- self.b*Z)   \n",
    "        return top/down\n",
    "    \n",
    "    def hyperbolic_tangent_differentiating(self,Z , a =1 , b = 1):\n",
    "        outer = b/a\n",
    "        y = self.hyperbolic_tangent(Z , b)\n",
    "        return outer*(a-y)*(a+y)\n",
    "    #%%\n",
    "    # select activation function\n",
    "    def activation(self , Z ,index,  a = 1):\n",
    "        if self.activation_function.lower() ==\"sigmoid\":\n",
    "            self.differentiating[index] = self.sigmoid_differentiating(Z)\n",
    "            return  self.sigmoid(Z , a)\n",
    "        elif self.activation_function.lower() ==\"hyperbolic_tangent\" :\n",
    "            self.differentiating[index] = self.hyperbolic_tangent_differentiating(Z)\n",
    "            return self.hyperbolic_tangent(Z , a)\n",
    "        else:\n",
    "            return Exception\n",
    "    # generate output\n",
    "    def train(self):\n",
    "        for j in range(self.neurons):\n",
    "              Z = np.dot(self.data , self.W[: , j])+self.bias[j]\n",
    "              try :\n",
    "                  self.a_out[j] =self.activation(Z , j)\n",
    "              except :\n",
    "                  self.a_out= np.zeros(self.neurons)\n",
    "                  print( \"Error in selected function\")\n",
    "                  return \n",
    "    def backpropagate(self):\n",
    "        pass \n",
    "    def forword_propagate_after_back(self):\n",
    "        pass\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer = hideen_Layer(2 , \"sigmoid\" , X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51661935, 0.70878298])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_layer.a_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58837525, 0.66808169])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_layer = hideen_Layer(2 , \"sigmoid\" , X)\n",
    "second_layer.train()\n",
    "second_layer.a_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- storge data of each layer in list\n",
    "- we have layer so we need number of layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sequence :\n",
    "    def __init__(self , num_of_layers ,epoch  , data):\n",
    "        self.layers = []\n",
    "        self.num_of_layers = num_of_layers\n",
    "        self.data  =data\n",
    "        self.epoch = epoch\n",
    "    def build_layers(self):\n",
    "        \n",
    "        # form GUI take units and activation\n",
    "        \n",
    "        self.layers.append(hideen_Layer(num_of_units , activation , self.data))\n",
    "        for i in range(self.num_of_layer-1):\n",
    "            previous_layer = self.layers[i]\n",
    "            self.layers.append(hideen_Layer(num_of_units , activation , previous_layer.a_out))\n",
    "        \n",
    "        previous_layer = self.layers[-1]\n",
    "        self.layers.append(hideen_Layer(3 , activation , previous_layer.a_out))\n",
    "        \n",
    "    def back_propagation(self):\n",
    "        len_of_layers = len(self.layers)-1\n",
    "        start_index = len_of_layers - 1\n",
    "\n",
    "        for i in range(len_of_layers):\n",
    "            for row in range(self.layers[start_index-i].neurons):\n",
    "                error_term=np.dot(self.layers[start_index-i+1].error , self.layers[start_index-i+1].W[row , :])\n",
    "                self.layers[start_index-i].error[row] =self.layers[start_index-i].differentiating[row]*error_term\n",
    "                \n",
    "    # decide where learning rate will put\n",
    "    def forward_propagation(self):\n",
    "        for layer in self.layers:\n",
    "            for neuron in range(layer.neurons):\n",
    "                layer.W[: , neuron] = layer.W[: , neuron] + learning_rate * layer.data * layer.error[neuron]\n",
    "                layer.bias[neuron] =  layer.bias[neuron] + learning_rate * layer.error[neuron]\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put input\n",
    "layers=[]\n",
    "num_of_layer = int(input(\"enter number of hidden layer\"))\n",
    "num_of_units = int(input(\"enter number of unints for unit 1\"))\n",
    "activation = input(\"enter activation function\")\n",
    "layers.append(hideen_Layer(num_of_units , activation , X))\n",
    "\n",
    "for i in range(num_of_layer-1):\n",
    "    num_of_units = int(input(f\"enter number of unints for unit {i+2}\"))\n",
    "    activation = input(\"enter activation function\")\n",
    "    previous_layer = layers[i]\n",
    "    layers.append(hideen_Layer(num_of_units , activation , previous_layer.a_out))\n",
    "    \n",
    "previous_layer = layers[-1]\n",
    "layers.append(hideen_Layer(1 , \"sigmoid\" , previous_layer.a_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layers :\n",
    "    for neuron in range(layer.neurons):\n",
    "        layer.W[: , neuron] = layer.W[: , neuron] + 0.5 * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32099001 0.21438743]\n",
      "[0.0498321  0.46760353]\n",
      "[0.56389822 0.70890522]\n",
      "[0.65312354 0.76950449]\n",
      "[0.36953099 0.64446893]\n",
      "done\n",
      "[0.02306508 0.92149792 0.05851145 0.03916407 0.42374823]\n",
      "[0.9685795  0.88186476 0.2765589  0.83877687 0.76463231]\n",
      "[0.47913084 0.2407692  0.29234205 0.24835575 0.3627783 ]\n",
      "[0.77305737 0.11990183 0.18884328 0.98809577 0.55524365]\n",
      "done\n",
      "[0.76860491 0.93967457 0.78471611 0.38553374]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for layer in layers :\n",
    "    for neuron in range(layer.neurons):\n",
    "        print(layer.W[: , neuron])\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[[0.32099001 0.0498321  0.56389822 0.65312354 0.36953099]\n",
      " [0.21438743 0.46760353 0.70890522 0.76950449 0.64446893]]\n",
      "[[0.02306508 0.9685795  0.47913084 0.77305737]\n",
      " [0.92149792 0.88186476 0.2407692  0.11990183]\n",
      " [0.05851145 0.2765589  0.29234205 0.18884328]\n",
      " [0.03916407 0.83877687 0.24835575 0.98809577]\n",
      " [0.42374823 0.76463231 0.3627783  0.55524365]]\n",
      "[[0.76860491]\n",
      " [0.93967457]\n",
      " [0.78471611]\n",
      " [0.38553374]]\n"
     ]
    }
   ],
   "source": [
    "print(len(layers))\n",
    "for i in range(len(layers)):\n",
    "    print(layers[i].W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "[0.64318728 0.53353054 0.56620692 0.69683459]\n",
      "[0.79079906 0.86500161]\n",
      "[0.72950588 0.7789295  0.67424649]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(layers)):\n",
    "    print(layers[i].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52389206 0.86620289 0.96367209 0.41114226]\n",
      " [0.39491797 0.15781282 0.72905456 0.22407595]]\n",
      "\n",
      "[[0.36149756 0.16380307]\n",
      " [0.7189759  0.85637456]\n",
      " [0.36072133 0.79438506]\n",
      " [0.17578366 0.00361792]]\n",
      "\n",
      "[[0.51366753 0.75617735 0.29173687]\n",
      " [0.55162012 0.07164045 0.18493112]]\n",
      "\n",
      "[[0.16311287]\n",
      " [0.44194065]\n",
      " [0.81963896]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(layers)):\n",
    "    print(layers[i].W)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.89050921e-05 1.78849194e-04 1.09066721e-04 2.71653824e-05]\n",
      "[0.00072681 0.00022895]\n",
      "[0.00111696 0.00264091 0.00624729]\n",
      "[0.03470254]\n"
     ]
    }
   ],
   "source": [
    "# print(len(layers))\n",
    "for i in range(len(layers)):\n",
    "    print(layers[i].error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52389206, 0.86620289, 0.96367209, 0.41114226])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[0].W[: , 0] # W1\n",
    "layers[0].W[0 , :] # W1[0] , W2[0] , ... Wn[0] where n in is num of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52389206 1.73240578 2.89101626 1.64456906]\n"
     ]
    }
   ],
   "source": [
    "print(layers[0].W[0,:] * [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.zeros(layers[-1].neurons)\n",
    "target[0] = 1\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z , a =1):\n",
    "    return (1/(1+math.exp(-a*Z)))\n",
    "\n",
    "def sigmoid_differentiating(y , a = 1):\n",
    "    return a*y*(1-y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03470254]\n"
     ]
    }
   ],
   "source": [
    "layers[-1].error=(target-layers[-1].a_out)*sigmoid_differentiating( layers[-1].a_out)\n",
    "print(layers[-1].error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "looping = len(layers)-1\n",
    "looping2 = looping-1\n",
    "for i in range(looping):\n",
    "    print(looping2-i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_of_layers = len(layers)-1\n",
    "start_index = len_of_layers - 1\n",
    "\n",
    "for i in range(len_of_layers):\n",
    "        for row in range(layers[start_index-i].neurons):\n",
    "                error_term=np.dot(layers[start_index-i+1].error , layers[start_index-i+1].W[row , :])\n",
    "                layers[start_index-i].error[row] = sigmoid_differentiating(  layers[start_index-i].a_out[row])*error_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64318728 0.53353054 0.56620692 0.69683459]\n",
      "\n",
      "[0.79079906 0.86500161]\n",
      "\n",
      "[0.72950588 0.7789295  0.67424649]\n",
      "\n",
      "[0.79047462]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(layers)):\n",
    "    print(layers[i].a_out)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52389206 0.86620289 0.96367209 0.41114226]\n",
      " [0.39491797 0.15781282 0.72905456 0.22407595]]\n",
      "\n",
      "[[0.36149756 0.16380307]\n",
      " [0.7189759  0.85637456]\n",
      " [0.36072133 0.79438506]\n",
      " [0.17578366 0.00361792]]\n",
      "\n",
      "[[0.51366753 0.75617735 0.29173687]\n",
      " [0.55162012 0.07164045 0.18493112]]\n",
      "\n",
      "[[0.16311287]\n",
      " [0.44194065]\n",
      " [0.81963896]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(layers)):\n",
    "    print(layers[i].W)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00022895257372403363\n"
     ]
    }
   ],
   "source": [
    "error_term = np.dot(layers[2].W[1 , :],layers[2].error)\n",
    "error_for_second = error_term * sigmoid_differentiating(layers[1].a_out[1])\n",
    "print(error_for_second)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.89050921e-05 1.78849194e-04 1.09066721e-04 2.71653824e-05]\n",
      "[0.00072681 0.00022895]\n",
      "[0.00111696 0.00264091 0.00624729]\n",
      "[0.03470254]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(layers)):\n",
    "    print(layers[i].error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[0.64318728 0.53353054 0.56620692 0.69683459]\n",
      "[0.79079906 0.86500161]\n",
      "[0.72950588 0.7789295  0.67424649]\n",
      "[0.79047462]\n"
     ]
    }
   ],
   "source": [
    "print(len(layers))\n",
    "print(layers[0].a_out)\n",
    "print(layers[1].a_out)\n",
    "print(layers[2].a_out)\n",
    "print(layers[3].a_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_output(y , target):\n",
    "    return (target[0] - y)*sigmoid_differentiating(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(W , bias , X):\n",
    "    units = W.shape[1] # shape 1 is number of col that indicate of neurans\n",
    "    a_out = np.zeros(units)\n",
    "    \n",
    "    for j in range(units):\n",
    "        Z = np.dot(W[:,j],X)+bias[j]\n",
    "        a_out[j] = sigmoid(Z)\n",
    "    return a_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42555748 0.5621765 ]\n"
     ]
    }
   ],
   "source": [
    "# first layer\n",
    "W = np.array([[0.21 , -0.4],\n",
    "              [0.15,0.1]])\n",
    "bias = np.array([-0.3,0.25])\n",
    "\n",
    "a_out= train(W,bias ,X)\n",
    "print(a_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42153907]\n"
     ]
    }
   ],
   "source": [
    "W_output = np.array([[-0.2],\n",
    "                    [0.3]])\n",
    "bias_out = np.array([-0.4])\n",
    "output_layer =train(W_output,bias_out ,a_out)\n",
    "print(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[ 0.21, -0.4 ],\n",
       "         [ 0.15,  0.1 ]]),\n",
       "  array([-0.3 ,  0.25]),\n",
       "  array([0.42555748, 0.5621765 ])],\n",
       " [array([[-0.2],\n",
       "         [ 0.3]]),\n",
       "  array([-0.4]),\n",
       "  array([0.42153907])]]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storge_data= [[W,bias,a_out], [W_output ,bias_out, output_layer]]\n",
    "storge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = np.array([0])\n",
    "error= []\n",
    "error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_output(W , bias , X):\n",
    "    units = W.shape[1] # shape 1 is number of col that indicate of neurans\n",
    "    a_out = np.zeros(units)\n",
    "    \n",
    "    for j in range(units):\n",
    "        Z = np.dot(W[:,j],X)+bias[j]\n",
    "        a_out[j] = Z\n",
    "    return a_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.10278972]\n"
     ]
    }
   ],
   "source": [
    "# y = np.dot(W_output , a_out) + bias_out\n",
    "print(calculate_error_output( output_layer, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net is W * X + bias\n",
    "# error for putput (target - output)*differentiating(net)\n",
    "# error in each unit = differentiating(net) *sum(error in previous *their W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_output(y , target):\n",
    "    return (target[0] - y)*sigmoid_differentiating(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
